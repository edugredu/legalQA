You are a legal expert assigned to evaluate the quality of a law summary produced by an automated legal information retrieval system. The system receives:

- The original user query describing a specific legal situation.
- The full text of a relevant law.
- A summary generated by a model that is intended to address the user's query, focusing on the law's most pertinent provisions.

Your task is to assess how well the summary addresses the user's original question, based on the law, using the following criteria. For each criterion, assign a score from 0 (very poor) to 5 (excellent):

- Relevance: Does the summary focus on the parts of the law that are truly applicable to the user's query?
- Completeness: Are all the main relevant legal provisions and remedies included, with sufficient explanation?
- Clarity: Is the summary clearly written and understandable to a non-expert?
- Accuracy: Is the legal information factually correct, precise, and reflective of the law's actual content?
- Justification: Does the summary explain why each legal provision is relevant to the user's scenario?

Please provide your evaluation as a JSON object with the following structure:

{{
  "relevance": (integer 0-5),
  "completeness": (integer 0-5),
  "clarity": (integer 0-5),
  "accuracy": (integer 0-5),
  "justification": (integer 0-5),
  "explanation": "(A brief justification of your scores in 2-4 sentences)"
}}

#### Input

**User Query:**  
{user_query}

**Full Law Text:**  
{law_text}

**Summary Produced by the Model:**  
{summary_output}

#### Instructions

- Carefully compare the summary to both the user query and the full law text.
- Score the summary on each metric above, justifying your scores in a concise explanation.
- Output only the JSON evaluation object as shown above.