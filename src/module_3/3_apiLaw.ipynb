{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "#from eurlex import get_data_by_celex_id #pip install eurlex-parser\n",
    "#from eurlex import get_html_by_celex_id #pip install eurlex\n",
    "import requests\n",
    "import pandas as pd\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_by_celex_id(celex_id: str) -> str:\n",
    "    \"\"\"Retrieve HTML by CELEX ID.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    celex_id : str\n",
    "        The CELEX ID to find HTML for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        HTML found using the CELEX ID.\n",
    "    \"\"\"\n",
    "    url = \"http://publications.europa.eu/resource/celex/\" + str(\n",
    "        celex_id\n",
    "    )  # pragma: no cover\n",
    "    response = requests.get(\n",
    "        url,\n",
    "        allow_redirects=True,\n",
    "        headers={  # pragma: no cover\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml\",  # pragma: no cover\n",
    "            \"Accept-Language\": \"en\",  # pragma: no cover\n",
    "        },\n",
    "    )  # pragma: no cover\n",
    "    html = response.content.decode(\"utf-8\")  # pragma: no cover\n",
    "    return html  # pragma: no cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL encode the celex_id\n",
    "def url_encode_celex_id(celex_id):\n",
    "    \"\"\"\n",
    "    URL encode the CELEX ID to ensure it is safe for web requests.\n",
    "    \n",
    "    Args:\n",
    "        celex_id (str): The CELEX ID to encode\n",
    "        \n",
    "    Returns:\n",
    "        str: URL encoded CELEX ID\n",
    "    \"\"\"\n",
    "    return urllib.parse.quote(celex_id, safe='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_by_celex_id_webservice(celex_id, username, password):\n",
    "    \"\"\"\n",
    "    Retrieve EU law document using EUR-Lex web service.\n",
    "    Handles special characters in CELEX IDs properly.\n",
    "    \"\"\"\n",
    "    # Escape special characters for EUR-Lex query syntax\n",
    "    escaped_celex_id = celex_id.replace('(', '\\\\(').replace(')', '\\\\)')\n",
    "    \n",
    "    soap_body = f\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<soap12:Envelope xmlns:soap12=\"http://www.w3.org/2003/05/soap-envelope\" \n",
    "                 xmlns:sear=\"http://eur-lex.europa.eu/search\"\n",
    "                 xmlns:wsse=\"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd\"\n",
    "                 xmlns:wsu=\"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-utility-1.0.xsd\">\n",
    "  <soap12:Header>\n",
    "    <wsse:Security soap12:mustUnderstand=\"true\">\n",
    "      <wsse:UsernameToken wsu:Id=\"UsernameToken-1\">\n",
    "        <wsse:Username>{username}</wsse:Username>\n",
    "        <wsse:Password Type=\"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-username-token-profile-1.0#PasswordText\">{password}</wsse:Password>\n",
    "      </wsse:UsernameToken>\n",
    "    </wsse:Security>\n",
    "  </soap12:Header>\n",
    "  <soap12:Body>\n",
    "    <sear:searchRequest>\n",
    "      <sear:expertQuery>DN={escaped_celex_id}</sear:expertQuery>\n",
    "      <sear:page>1</sear:page>\n",
    "      <sear:pageSize>1</sear:pageSize>\n",
    "      <sear:searchLanguage>en</sear:searchLanguage>\n",
    "    </sear:searchRequest>\n",
    "  </soap12:Body>\n",
    "</soap12:Envelope>\"\"\"\n",
    "    \n",
    "    headers = {\n",
    "        'Content-Type': 'application/soap+xml; charset=utf-8',\n",
    "        'SOAPAction': 'https://eur-lex.europa.eu/EURLexWebService/doQuery'\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        'https://eur-lex.europa.eu/EURLexWebService',\n",
    "        data=soap_body,\n",
    "        headers=headers,\n",
    "        timeout=30\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Response status: {response.status_code}\")\n",
    "        print(f\"Response content: {response.text}\")\n",
    "        raise Exception(f\"SOAP request failed: {response.status_code}\")\n",
    "    \n",
    "    return parse_search_results(response.text)\n",
    "\n",
    "def parse_search_results(soap_response):\n",
    "    \"\"\"Parse the SOAP response to extract search results\"\"\"\n",
    "    import xml.etree.ElementTree as ET\n",
    "    \n",
    "    try:\n",
    "        root = ET.fromstring(soap_response)\n",
    "        \n",
    "        # Define namespaces\n",
    "        namespaces = {\n",
    "            'soap12': 'http://www.w3.org/2003/05/soap-envelope',\n",
    "            'elx': 'http://eur-lex.europa.eu/search'\n",
    "        }\n",
    "        \n",
    "        # Find search results\n",
    "        search_results = root.find('.//elx:searchResults', namespaces)\n",
    "        \n",
    "        if search_results is not None:\n",
    "            # Extract document information\n",
    "            documents = search_results.findall('.//elx:document', namespaces)\n",
    "            return documents\n",
    "        else:\n",
    "            raise Exception(\"No search results found in response\")\n",
    "            \n",
    "    except ET.ParseError as e:\n",
    "        raise Exception(f\"Failed to parse SOAP response: {e}\")\n",
    "\n",
    "def extract_eu_law_text(html_content):\n",
    "    \"\"\"\n",
    "    Extract clean, structured text from EU legal document HTML.\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): Raw HTML content of the EU law document\n",
    "        \n",
    "    Returns:\n",
    "        str: Clean, structured text with titles, articles, and appendices\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Remove unwanted elements\n",
    "    for element in soup(['script', 'style', 'link', 'meta', 'hr']):\n",
    "        element.decompose()\n",
    "    \n",
    "    extracted_text = []\n",
    "    \n",
    "    # Extract document header information\n",
    "    header_info = extract_header_info(soup)\n",
    "    if header_info:\n",
    "        extracted_text.append(header_info)\n",
    "    \n",
    "    # Extract main title and regulation info\n",
    "    main_title = extract_main_title(soup)\n",
    "    if main_title:\n",
    "        extracted_text.append(main_title)\n",
    "    \n",
    "    # Extract preamble/whereas clauses\n",
    "    preamble = extract_preamble(soup)\n",
    "    if preamble:\n",
    "        extracted_text.append(preamble)\n",
    "    \n",
    "    # Extract main articles\n",
    "    articles = extract_articles(soup)\n",
    "    if articles:\n",
    "        extracted_text.append(articles)\n",
    "    \n",
    "    # Extract annexes\n",
    "    annexes = extract_annexes(soup)\n",
    "    if annexes:\n",
    "        extracted_text.append(annexes)\n",
    "    \n",
    "    # Extract appendices\n",
    "    appendices = extract_appendices(soup)\n",
    "    if appendices:\n",
    "        extracted_text.append(appendices)\n",
    "    \n",
    "    return '\\n\\n'.join(filter(None, extracted_text))\n",
    "\n",
    "def extract_header_info(soup):\n",
    "    \"\"\"Extract document header with date and publication info\"\"\"\n",
    "    header_text = []\n",
    "    \n",
    "    # Extract date and language info\n",
    "    header_table = soup.find('table', width=\"100%\")\n",
    "    if header_table:\n",
    "        for row in header_table.find_all('tr'):\n",
    "            cells = row.find_all('td')\n",
    "            if cells:\n",
    "                row_text = ' | '.join(cell.get_text(strip=True) for cell in cells if cell.get_text(strip=True))\n",
    "                if row_text:\n",
    "                    header_text.append(row_text)\n",
    "    \n",
    "    return '\\n'.join(header_text) if header_text else None\n",
    "\n",
    "def extract_main_title(soup):\n",
    "    \"\"\"Extract the main regulation title and basic info\"\"\"\n",
    "    title_text = []\n",
    "    \n",
    "    # Main title\n",
    "    main_title = soup.find(class_='eli-main-title')\n",
    "    if main_title:\n",
    "        for p in main_title.find_all('p'):\n",
    "            text = p.get_text(strip=True)\n",
    "            if text:\n",
    "                title_text.append(text)\n",
    "    \n",
    "    return '\\n'.join(title_text) if title_text else None\n",
    "\n",
    "def extract_preamble(soup):\n",
    "    \"\"\"Extract the preamble including 'Whereas' clauses\"\"\"\n",
    "    preamble_text = []\n",
    "    \n",
    "    # Look for preamble section\n",
    "    preamble_section = soup.find('div', id='pbl_1')\n",
    "    if preamble_section:\n",
    "        preamble_text.append(\"PREAMBLE\")\n",
    "        \n",
    "        # Extract \"THE EUROPEAN COMMISSION\" and \"Having regard to\" sections\n",
    "        for p in preamble_section.find_all('p', class_='oj-normal', recursive=False):\n",
    "            text = p.get_text(strip=True)\n",
    "            if text:\n",
    "                preamble_text.append(text)\n",
    "        \n",
    "        # Extract \"Whereas\" clauses (recitals)\n",
    "        whereas_clauses = preamble_section.find_all('div', id=re.compile(r'rct_\\d+'))\n",
    "        if whereas_clauses:\n",
    "            preamble_text.append(\"\\nWHEREAS:\")\n",
    "            for clause in whereas_clauses:\n",
    "                table = clause.find('table')\n",
    "                if table:\n",
    "                    rows = table.find_all('tr')\n",
    "                    for row in rows:\n",
    "                        cells = row.find_all('td')\n",
    "                        if len(cells) >= 2:\n",
    "                            number = cells[0].get_text(strip=True)\n",
    "                            content = cells[1].get_text(strip=True)\n",
    "                            if number and content:\n",
    "                                preamble_text.append(f\"({number}) {content}\")\n",
    "    \n",
    "    return '\\n\\n'.join(preamble_text) if preamble_text else None\n",
    "\n",
    "def extract_articles(soup):\n",
    "    \"\"\"Extract all articles with their content\"\"\"\n",
    "    articles_text = []\n",
    "    \n",
    "    # Find all article divisions\n",
    "    articles = soup.find_all('div', id=re.compile(r'art_\\d+'))\n",
    "    \n",
    "    for article in articles:\n",
    "        article_content = []\n",
    "        \n",
    "        # Article title\n",
    "        article_title = article.find(class_='oj-ti-art')\n",
    "        if article_title:\n",
    "            article_content.append(article_title.get_text(strip=True))\n",
    "        \n",
    "        # Article subtitle\n",
    "        article_subtitle = article.find(class_='oj-sti-art')\n",
    "        if article_subtitle:\n",
    "            article_content.append(article_subtitle.get_text(strip=True))\n",
    "        \n",
    "        # Article paragraphs\n",
    "        paragraphs = extract_article_paragraphs(article)\n",
    "        if paragraphs:\n",
    "            article_content.extend(paragraphs)\n",
    "        \n",
    "        if article_content:\n",
    "            articles_text.append('\\n'.join(article_content))\n",
    "    \n",
    "    return '\\n\\n'.join(articles_text) if articles_text else None\n",
    "\n",
    "def extract_article_paragraphs(article):\n",
    "    \"\"\"Extract paragraphs and structured content from an article\"\"\"\n",
    "    paragraphs = []\n",
    "    \n",
    "    # Direct paragraphs\n",
    "    for p in article.find_all('p', class_='oj-normal'):\n",
    "        text = p.get_text(strip=True)\n",
    "        if text and not text.startswith('(') and ')' not in text[:5]:\n",
    "            paragraphs.append(text)\n",
    "    \n",
    "    # Numbered paragraphs in tables\n",
    "    for table in article.find_all('table'):\n",
    "        rows = table.find_all('tr')\n",
    "        for row in rows:\n",
    "            cells = row.find_all('td')\n",
    "            if len(cells) >= 2:\n",
    "                number = cells[0].get_text(strip=True)\n",
    "                content = cells[1].get_text(strip=True)\n",
    "                if number and content and re.match(r'\\([a-z0-9]+\\)', number):\n",
    "                    paragraphs.append(f\"{number} {content}\")\n",
    "    \n",
    "    # Numbered divisions within article\n",
    "    for div in article.find_all('div', id=re.compile(r'\\d{3}\\.\\d{3}')):\n",
    "        div_paragraphs = div.find_all('p', class_='oj-normal')\n",
    "        for p in div_paragraphs:\n",
    "            text = p.get_text(strip=True)\n",
    "            if text:\n",
    "                paragraphs.append(text)\n",
    "        \n",
    "        # Tables within divisions\n",
    "        for table in div.find_all('table'):\n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows:\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) >= 2:\n",
    "                    number = cells[0].get_text(strip=True)\n",
    "                    content = cells[1].get_text(strip=True)\n",
    "                    if number and content:\n",
    "                        paragraphs.append(f\"{number} {content}\")\n",
    "    \n",
    "    return paragraphs\n",
    "\n",
    "def extract_annexes(soup):\n",
    "    \"\"\"Extract annex content\"\"\"\n",
    "    annex_text = []\n",
    "    \n",
    "    # Find annex container\n",
    "    annex = soup.find('div', id='anx_1')\n",
    "    if annex:\n",
    "        # Annex title\n",
    "        annex_title = annex.find('p', class_='oj-doc-ti')\n",
    "        if annex_title:\n",
    "            annex_text.append(f\"ANNEX\\n{annex_title.get_text(strip=True)}\")\n",
    "        \n",
    "        # Extract all sections within annex\n",
    "        sections = extract_annex_sections(annex)\n",
    "        if sections:\n",
    "            annex_text.extend(sections)\n",
    "    \n",
    "    return '\\n\\n'.join(annex_text) if annex_text else None\n",
    "\n",
    "def extract_annex_sections(annex):\n",
    "    \"\"\"Extract sections from annex (Parts A, B, C, etc.)\"\"\"\n",
    "    sections = []\n",
    "    \n",
    "    # Find all part titles and UAS sections\n",
    "    part_titles = annex.find_all('p', class_='oj-ti-grseq-1')\n",
    "    \n",
    "    current_section = []\n",
    "    \n",
    "    for element in annex.find_all(['p', 'table']):\n",
    "        if element.name == 'p':\n",
    "            text = element.get_text(strip=True)\n",
    "            if text:\n",
    "                # Check if it's a section title\n",
    "                if 'PART' in text or 'UAS.' in text:\n",
    "                    if current_section:\n",
    "                        sections.append('\\n'.join(current_section))\n",
    "                        current_section = []\n",
    "                    current_section.append(text)\n",
    "                else:\n",
    "                    current_section.append(text)\n",
    "        \n",
    "        elif element.name == 'table':\n",
    "            # Extract table content\n",
    "            rows = element.find_all('tr')\n",
    "            for row in rows:\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) >= 2:\n",
    "                    number = cells[0].get_text(strip=True)\n",
    "                    content = cells[1].get_text(strip=True)\n",
    "                    if number and content:\n",
    "                        current_section.append(f\"{number} {content}\")\n",
    "    \n",
    "    # Add the last section\n",
    "    if current_section:\n",
    "        sections.append('\\n'.join(current_section))\n",
    "    \n",
    "    return sections\n",
    "\n",
    "def extract_appendices(soup):\n",
    "    \"\"\"Extract appendix content\"\"\"\n",
    "    appendix_text = []\n",
    "    \n",
    "    # Find appendix\n",
    "    appendix = soup.find('div', id='anx_1.app_1')\n",
    "    if appendix:\n",
    "        appendix_title = appendix.find('p', class_='oj-doc-ti')\n",
    "        if appendix_title:\n",
    "            appendix_text.append(f\"APPENDIX\\n{appendix_title.get_text(strip=True)}\")\n",
    "        \n",
    "        # Extract appendix content\n",
    "        for p in appendix.find_all('p'):\n",
    "            text = p.get_text(strip=True)\n",
    "            if text and text not in appendix_text:\n",
    "                appendix_text.append(text)\n",
    "    \n",
    "    return '\\n\\n'.join(appendix_text) if appendix_text else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load to a dataframe the lawsToBeConsidered.csv\n",
    "df = pd.read_csv('lawsToBeConsidered.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for CELEX ID: 32019R0980\n",
      "Looking for CELEX ID: 32019D0785\n",
      "Looking for CELEX ID: 32019R1122\n",
      "Looking for CELEX ID: 32019R0856\n",
      "Looking for CELEX ID: 22020A0724(01)\n"
     ]
    }
   ],
   "source": [
    "for i, row in df.iterrows():\n",
    "    #Get the CELEX ID\n",
    "    celex_id = row['celex_id']\n",
    "\n",
    "\n",
    "    #If the CELEX ID contains (), remove it. For example, '32023R1234(1)' should become '32023R1234'\n",
    "    #celex_id = re.sub(r'\\(\\d+\\)$', '', celex_id)\n",
    "\n",
    "    encoded_celex_id = url_encode_celex_id(celex_id)\n",
    "    \n",
    "    print(\"Looking for CELEX ID:\", celex_id)\n",
    "\n",
    "    #Get the HTML content\n",
    "    html = get_html_by_celex_id(encoded_celex_id)\n",
    "\n",
    "    # Extract structured text\n",
    "    structured_text = extract_eu_law_text(html)\n",
    "\n",
    "    # Append the structured text to the DataFrame\n",
    "    df.at[i, 'structured_text'] = structured_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the dataframe to a CSV file\n",
    "df.to_csv('lawsWithText.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
