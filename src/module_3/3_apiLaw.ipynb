{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "#from eurlex import get_data_by_celex_id #pip install eurlex-parser\n",
    "#from eurlex import get_html_by_celex_id #pip install eurlex\n",
    "import requests\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_by_celex_id(celex_id: str) -> str:\n",
    "    \"\"\"Retrieve HTML by CELEX ID.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    celex_id : str\n",
    "        The CELEX ID to find HTML for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        HTML found using the CELEX ID.\n",
    "    \"\"\"\n",
    "    url = \"http://publications.europa.eu/resource/celex/\" + str(\n",
    "        celex_id\n",
    "    )  # pragma: no cover\n",
    "    response = requests.get(\n",
    "        url,\n",
    "        allow_redirects=True,\n",
    "        headers={  # pragma: no cover\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml\",  # pragma: no cover\n",
    "            \"Accept-Language\": \"en\",  # pragma: no cover\n",
    "        },\n",
    "    )  # pragma: no cover\n",
    "    html = response.content.decode(\"utf-8\")  # pragma: no cover\n",
    "    return html  # pragma: no cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL encode the celex_id\n",
    "def url_encode_celex_id(celex_id):\n",
    "    \"\"\"\n",
    "    URL encode the CELEX ID to ensure it is safe for web requests.\n",
    "    \n",
    "    Args:\n",
    "        celex_id (str): The CELEX ID to encode\n",
    "        \n",
    "    Returns:\n",
    "        str: URL encoded CELEX ID\n",
    "    \"\"\"\n",
    "    return urllib.parse.quote(celex_id, safe='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_search_results(soap_response):\n",
    "    \"\"\"Parse the SOAP response to extract search results\"\"\"\n",
    "    import xml.etree.ElementTree as ET\n",
    "    \n",
    "    try:\n",
    "        root = ET.fromstring(soap_response)\n",
    "        \n",
    "        # Define namespaces\n",
    "        namespaces = {\n",
    "            'soap12': 'http://www.w3.org/2003/05/soap-envelope',\n",
    "            'elx': 'http://eur-lex.europa.eu/search'\n",
    "        }\n",
    "        \n",
    "        # Find search results\n",
    "        search_results = root.find('.//elx:searchResults', namespaces)\n",
    "        \n",
    "        if search_results is not None:\n",
    "            # Extract document information\n",
    "            documents = search_results.findall('.//elx:document', namespaces)\n",
    "            return documents\n",
    "        else:\n",
    "            raise Exception(\"No search results found in response\")\n",
    "            \n",
    "    except ET.ParseError as e:\n",
    "        raise Exception(f\"Failed to parse SOAP response: {e}\")\n",
    "\n",
    "def extract_eu_law_text(html_content):\n",
    "    \"\"\"\n",
    "    Extract clean, structured text from EU legal document HTML.\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): Raw HTML content of the EU law document\n",
    "        \n",
    "    Returns:\n",
    "        str: Clean, structured text with titles, articles, and appendices\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Remove unwanted elements\n",
    "    for element in soup(['script', 'style', 'link', 'meta', 'hr']):\n",
    "        element.decompose()\n",
    "    \n",
    "    extracted_text = []\n",
    "    \n",
    "    # Extract document header information\n",
    "    header_info = extract_header_info(soup)\n",
    "    if header_info:\n",
    "        extracted_text.append(header_info)\n",
    "    \n",
    "    # Extract main title and regulation info\n",
    "    main_title = extract_main_title(soup)\n",
    "    if main_title:\n",
    "        extracted_text.append(main_title)\n",
    "    \n",
    "    # Extract preamble/whereas clauses\n",
    "    preamble = extract_preamble(soup)\n",
    "    if preamble:\n",
    "        extracted_text.append(preamble)\n",
    "    \n",
    "    # Extract main articles\n",
    "    articles = extract_articles(soup)\n",
    "    if articles:\n",
    "        extracted_text.append(articles)\n",
    "    \n",
    "    # Extract annexes\n",
    "    annexes = extract_annexes(soup)\n",
    "    if annexes:\n",
    "        extracted_text.append(annexes)\n",
    "    \n",
    "    # Extract appendices\n",
    "    appendices = extract_appendices(soup)\n",
    "    if appendices:\n",
    "        extracted_text.append(appendices)\n",
    "    \n",
    "    return '\\n\\n'.join(filter(None, extracted_text))\n",
    "\n",
    "def extract_header_info(soup):\n",
    "    \"\"\"Extract document header with date and publication info\"\"\"\n",
    "    header_text = []\n",
    "    \n",
    "    # Extract date and language info\n",
    "    header_table = soup.find('table', width=\"100%\")\n",
    "    if header_table:\n",
    "        for row in header_table.find_all('tr'):\n",
    "            cells = row.find_all('td')\n",
    "            if cells:\n",
    "                row_text = ' | '.join(cell.get_text(strip=True) for cell in cells if cell.get_text(strip=True))\n",
    "                if row_text:\n",
    "                    header_text.append(row_text)\n",
    "    \n",
    "    return '\\n'.join(header_text) if header_text else None\n",
    "\n",
    "def extract_main_title(soup):\n",
    "    \"\"\"Extract the main regulation title and basic info\"\"\"\n",
    "    title_text = []\n",
    "    \n",
    "    # Main title\n",
    "    main_title = soup.find(class_='eli-main-title')\n",
    "    if main_title:\n",
    "        for p in main_title.find_all('p'):\n",
    "            text = p.get_text(strip=True)\n",
    "            if text:\n",
    "                title_text.append(text)\n",
    "    \n",
    "    return '\\n'.join(title_text) if title_text else None\n",
    "\n",
    "def extract_preamble(soup):\n",
    "    \"\"\"Extract the preamble including 'Whereas' clauses\"\"\"\n",
    "    preamble_text = []\n",
    "    \n",
    "    # Look for preamble section\n",
    "    preamble_section = soup.find('div', id='pbl_1')\n",
    "    if preamble_section:\n",
    "        preamble_text.append(\"PREAMBLE\")\n",
    "        \n",
    "        # Extract \"THE EUROPEAN COMMISSION\" and \"Having regard to\" sections\n",
    "        for p in preamble_section.find_all('p', class_='oj-normal', recursive=False):\n",
    "            text = p.get_text(strip=True)\n",
    "            if text:\n",
    "                preamble_text.append(text)\n",
    "        \n",
    "        # Extract \"Whereas\" clauses (recitals)\n",
    "        whereas_clauses = preamble_section.find_all('div', id=re.compile(r'rct_\\d+'))\n",
    "        if whereas_clauses:\n",
    "            preamble_text.append(\"\\nWHEREAS:\")\n",
    "            for clause in whereas_clauses:\n",
    "                table = clause.find('table')\n",
    "                if table:\n",
    "                    rows = table.find_all('tr')\n",
    "                    for row in rows:\n",
    "                        cells = row.find_all('td')\n",
    "                        if len(cells) >= 2:\n",
    "                            number = cells[0].get_text(strip=True)\n",
    "                            content = cells[1].get_text(strip=True)\n",
    "                            if number and content:\n",
    "                                preamble_text.append(f\"({number}) {content}\")\n",
    "    \n",
    "    return '\\n\\n'.join(preamble_text) if preamble_text else None\n",
    "\n",
    "def extract_articles(soup):\n",
    "    \"\"\"Extract all articles with their content\"\"\"\n",
    "    articles_text = []\n",
    "    \n",
    "    # Find all article divisions\n",
    "    articles = soup.find_all('div', id=re.compile(r'art_\\d+'))\n",
    "    \n",
    "    for article in articles:\n",
    "        article_content = []\n",
    "        \n",
    "        # Article title\n",
    "        article_title = article.find(class_='oj-ti-art')\n",
    "        if article_title:\n",
    "            article_content.append(article_title.get_text(strip=True))\n",
    "        \n",
    "        # Article subtitle\n",
    "        article_subtitle = article.find(class_='oj-sti-art')\n",
    "        if article_subtitle:\n",
    "            article_content.append(article_subtitle.get_text(strip=True))\n",
    "        \n",
    "        # Article paragraphs\n",
    "        paragraphs = extract_article_paragraphs(article)\n",
    "        if paragraphs:\n",
    "            article_content.extend(paragraphs)\n",
    "        \n",
    "        if article_content:\n",
    "            articles_text.append('\\n'.join(article_content))\n",
    "    \n",
    "    return '\\n\\n'.join(articles_text) if articles_text else None\n",
    "\n",
    "def extract_article_paragraphs(article):\n",
    "    \"\"\"Extract paragraphs and structured content from an article\"\"\"\n",
    "    paragraphs = []\n",
    "    \n",
    "    # Direct paragraphs\n",
    "    for p in article.find_all('p', class_='oj-normal'):\n",
    "        text = p.get_text(strip=True)\n",
    "        if text and not text.startswith('(') and ')' not in text[:5]:\n",
    "            paragraphs.append(text)\n",
    "    \n",
    "    # Numbered paragraphs in tables\n",
    "    for table in article.find_all('table'):\n",
    "        rows = table.find_all('tr')\n",
    "        for row in rows:\n",
    "            cells = row.find_all('td')\n",
    "            if len(cells) >= 2:\n",
    "                number = cells[0].get_text(strip=True)\n",
    "                content = cells[1].get_text(strip=True)\n",
    "                if number and content and re.match(r'\\([a-z0-9]+\\)', number):\n",
    "                    paragraphs.append(f\"{number} {content}\")\n",
    "    \n",
    "    # Numbered divisions within article\n",
    "    for div in article.find_all('div', id=re.compile(r'\\d{3}\\.\\d{3}')):\n",
    "        div_paragraphs = div.find_all('p', class_='oj-normal')\n",
    "        for p in div_paragraphs:\n",
    "            text = p.get_text(strip=True)\n",
    "            if text:\n",
    "                paragraphs.append(text)\n",
    "        \n",
    "        # Tables within divisions\n",
    "        for table in div.find_all('table'):\n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows:\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) >= 2:\n",
    "                    number = cells[0].get_text(strip=True)\n",
    "                    content = cells[1].get_text(strip=True)\n",
    "                    if number and content:\n",
    "                        paragraphs.append(f\"{number} {content}\")\n",
    "    \n",
    "    return paragraphs\n",
    "\n",
    "def extract_annexes(soup):\n",
    "    \"\"\"Extract annex content\"\"\"\n",
    "    annex_text = []\n",
    "    \n",
    "    # Find annex container\n",
    "    annex = soup.find('div', id='anx_1')\n",
    "    if annex:\n",
    "        # Annex title\n",
    "        annex_title = annex.find('p', class_='oj-doc-ti')\n",
    "        if annex_title:\n",
    "            annex_text.append(f\"ANNEX\\n{annex_title.get_text(strip=True)}\")\n",
    "        \n",
    "        # Extract all sections within annex\n",
    "        sections = extract_annex_sections(annex)\n",
    "        if sections:\n",
    "            annex_text.extend(sections)\n",
    "    \n",
    "    return '\\n\\n'.join(annex_text) if annex_text else None\n",
    "\n",
    "def extract_annex_sections(annex):\n",
    "    \"\"\"Extract sections from annex (Parts A, B, C, etc.)\"\"\"\n",
    "    sections = []\n",
    "    \n",
    "    # Find all part titles and UAS sections\n",
    "    part_titles = annex.find_all('p', class_='oj-ti-grseq-1')\n",
    "    \n",
    "    current_section = []\n",
    "    \n",
    "    for element in annex.find_all(['p', 'table']):\n",
    "        if element.name == 'p':\n",
    "            text = element.get_text(strip=True)\n",
    "            if text:\n",
    "                # Check if it's a section title\n",
    "                if 'PART' in text or 'UAS.' in text:\n",
    "                    if current_section:\n",
    "                        sections.append('\\n'.join(current_section))\n",
    "                        current_section = []\n",
    "                    current_section.append(text)\n",
    "                else:\n",
    "                    current_section.append(text)\n",
    "        \n",
    "        elif element.name == 'table':\n",
    "            # Extract table content\n",
    "            rows = element.find_all('tr')\n",
    "            for row in rows:\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) >= 2:\n",
    "                    number = cells[0].get_text(strip=True)\n",
    "                    content = cells[1].get_text(strip=True)\n",
    "                    if number and content:\n",
    "                        current_section.append(f\"{number} {content}\")\n",
    "    \n",
    "    # Add the last section\n",
    "    if current_section:\n",
    "        sections.append('\\n'.join(current_section))\n",
    "    \n",
    "    return sections\n",
    "\n",
    "def extract_appendices(soup):\n",
    "    \"\"\"Extract appendix content\"\"\"\n",
    "    appendix_text = []\n",
    "    \n",
    "    # Find appendix\n",
    "    appendix = soup.find('div', id='anx_1.app_1')\n",
    "    if appendix:\n",
    "        appendix_title = appendix.find('p', class_='oj-doc-ti')\n",
    "        if appendix_title:\n",
    "            appendix_text.append(f\"APPENDIX\\n{appendix_title.get_text(strip=True)}\")\n",
    "        \n",
    "        # Extract appendix content\n",
    "        for p in appendix.find_all('p'):\n",
    "            text = p.get_text(strip=True)\n",
    "            if text and text not in appendix_text:\n",
    "                appendix_text.append(text)\n",
    "    \n",
    "    return '\\n\\n'.join(appendix_text) if appendix_text else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_articles_json(soup):\n",
    "    \"\"\"Extract all articles as a list of dictionaries with full text in a single property\"\"\"\n",
    "    articles_list = []\n",
    "    \n",
    "    # Find all article divisions\n",
    "    articles = soup.find_all('div', id=re.compile(r'art_\\d+'))\n",
    "    \n",
    "    for article in articles:\n",
    "        article_data = {}\n",
    "        \n",
    "        # Extract article ID from the div id\n",
    "        article_id = article.get('id', '').replace('art_', '')\n",
    "        if article_id:\n",
    "            article_data['id'] = article_id\n",
    "        \n",
    "        # Article title\n",
    "        article_title = article.find(class_='oj-ti-art')\n",
    "        if article_title:\n",
    "            article_data['title'] = article_title.get_text(strip=True)\n",
    "        \n",
    "        # Article subtitle\n",
    "        article_subtitle = article.find(class_='oj-sti-art')\n",
    "        if article_subtitle:\n",
    "            article_data['subtitle'] = article_subtitle.get_text(strip=True)\n",
    "        \n",
    "        # Get full text content of the article div\n",
    "        full_text = article.get_text(separator='\\n', strip=True)\n",
    "        article_data['text'] = full_text\n",
    "        \n",
    "        articles_list.append(article_data)\n",
    "    \n",
    "    return articles_list if articles_list else None\n",
    "\n",
    "def extract_annexes_json(soup):\n",
    "    \"\"\"Extract annex content as structured data with full text in a single property\"\"\"\n",
    "    annexes_list = []\n",
    "    \n",
    "    # Find annex container\n",
    "    annex = soup.find('div', id='anx_1')\n",
    "    if annex:\n",
    "        annex_data = {\n",
    "            \"id\": \"anx_1\",\n",
    "            \"type\": \"annex\"\n",
    "        }\n",
    "        \n",
    "        # Annex title\n",
    "        annex_title = annex.find('p', class_='oj-doc-ti')\n",
    "        if annex_title:\n",
    "            annex_data['title'] = annex_title.get_text(strip=True)\n",
    "        \n",
    "        # Get full text content of the annex div\n",
    "        full_text = annex.get_text(separator='\\n', strip=True)\n",
    "        annex_data['text'] = full_text\n",
    "        \n",
    "        annexes_list.append(annex_data)\n",
    "    \n",
    "    return annexes_list if annexes_list else None\n",
    "\n",
    "def extract_appendices_json(soup):\n",
    "    \"\"\"Extract appendix content as structured data with full text in a single property\"\"\"\n",
    "    appendices_list = []\n",
    "    \n",
    "    # Find appendix\n",
    "    appendix = soup.find('div', id='anx_1.app_1')\n",
    "    if appendix:\n",
    "        appendix_data = {\n",
    "            \"id\": \"anx_1.app_1\",\n",
    "            \"type\": \"appendix\"\n",
    "        }\n",
    "        \n",
    "        appendix_title = appendix.find('p', class_='oj-doc-ti')\n",
    "        if appendix_title:\n",
    "            appendix_data['title'] = appendix_title.get_text(strip=True)\n",
    "        \n",
    "        # Get full text content of the appendix div\n",
    "        full_text = appendix.get_text(separator='\\n', strip=True)\n",
    "        appendix_data['text'] = full_text\n",
    "        \n",
    "        appendices_list.append(appendix_data)\n",
    "    \n",
    "    return appendices_list if appendices_list else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_legacy_format(soup, texte_only_div):\n",
    "    \"\"\"\n",
    "    Parse the older EUR-Lex HTML format found in TexteOnly div.\n",
    "    \"\"\"\n",
    "    # Extract title from outside TexteOnly div\n",
    "    title_element = soup.find('h1')\n",
    "    title = title_element.get_text(strip=True) if title_element else None\n",
    "    \n",
    "    # Extract the main regulation title\n",
    "    strong_element = soup.find('strong')\n",
    "    regulation_title = strong_element.get_text(strip=True) if strong_element else None\n",
    "    \n",
    "    # Get all paragraphs within TexteOnly\n",
    "    paragraphs = texte_only_div.find_all('p')\n",
    "    \n",
    "    # Parse the content\n",
    "    articles = []\n",
    "    annexes = []\n",
    "    current_article = None\n",
    "    current_annex = None\n",
    "    \n",
    "    for p in paragraphs:\n",
    "        text = p.get_text(strip=True)\n",
    "        \n",
    "        if not text:\n",
    "            continue\n",
    "            \n",
    "        # Check if this is an article\n",
    "        if text.startswith('Article '):\n",
    "            # Save previous article if exists\n",
    "            if current_article:\n",
    "                articles.append(current_article)\n",
    "            \n",
    "            # Extract article number\n",
    "            article_match = re.match(r'Article (\\d+)', text)\n",
    "            if article_match:\n",
    "                current_article = {\n",
    "                    \"id\": article_match.group(1),\n",
    "                    \"title\": text,\n",
    "                    \"text\": text\n",
    "                }\n",
    "            current_annex = None\n",
    "            \n",
    "        # Check if this is an annex\n",
    "        elif text.startswith('ANNEX '):\n",
    "            # Save previous article if exists\n",
    "            if current_article:\n",
    "                articles.append(current_article)\n",
    "                current_article = None\n",
    "            \n",
    "            # Save previous annex if exists\n",
    "            if current_annex:\n",
    "                annexes.append(current_annex)\n",
    "            \n",
    "            # Extract annex identifier\n",
    "            annex_match = re.match(r'ANNEX ([IVX]+)', text)\n",
    "            if annex_match:\n",
    "                current_annex = {\n",
    "                    \"id\": f\"anx_{annex_match.group(1)}\",\n",
    "                    \"type\": \"annex\",\n",
    "                    \"title\": text,\n",
    "                    \"text\": text\n",
    "                }\n",
    "            \n",
    "        # Add content to current article or annex\n",
    "        elif current_article:\n",
    "            current_article[\"text\"] += \"\\n\\n\" + text\n",
    "        elif current_annex:\n",
    "            current_annex[\"text\"] += \"\\n\\n\" + text\n",
    "    \n",
    "    # Don't forget the last article/annex\n",
    "    if current_article:\n",
    "        articles.append(current_article)\n",
    "    if current_annex:\n",
    "        annexes.append(current_annex)\n",
    "    \n",
    "    # Build the document structure\n",
    "    document_data = {\n",
    "        \"title\": regulation_title or title,\n",
    "        \"articles\": articles if articles else None,\n",
    "        \"annexes\": annexes if annexes else None\n",
    "    }\n",
    "    \n",
    "    return document_data\n",
    "\n",
    "def parse_legacy_format_enhanced(soup, texte_only_div):\n",
    "    \"\"\"\n",
    "    Enhanced parser for older EUR-Lex HTML format with better content detection.\n",
    "    \"\"\"\n",
    "    # Extract metadata\n",
    "    title_element = soup.find('h1')\n",
    "    title = title_element.get_text(strip=True) if title_element else None\n",
    "    \n",
    "    strong_element = soup.find('strong')\n",
    "    regulation_title = strong_element.get_text(strip=True) if strong_element else None\n",
    "    \n",
    "    # Get full text content and split into logical sections\n",
    "    full_text = texte_only_div.get_text(separator='\\n', strip=True)\n",
    "    \n",
    "    # Split by known patterns\n",
    "    articles = []\n",
    "    annexes = []\n",
    "    \n",
    "    # Find all article sections\n",
    "    article_pattern = r'Article (\\d+)\\s*\\n(.*?)(?=Article \\d+|ANNEX|$)'\n",
    "    article_matches = re.findall(article_pattern, full_text, re.DOTALL)\n",
    "    \n",
    "    for article_num, article_content in article_matches:\n",
    "        articles.append({\n",
    "            \"id\": article_num,\n",
    "            \"title\": f\"Article {article_num}\",\n",
    "            \"text\": f\"Article {article_num}\\n\\n{article_content.strip()}\"\n",
    "        })\n",
    "    \n",
    "    # Find all annex sections\n",
    "    annex_pattern = r'ANNEX ([IVX]+)\\s*\\n(.*?)(?=ANNEX [IVX]+|$)'\n",
    "    annex_matches = re.findall(annex_pattern, full_text, re.DOTALL)\n",
    "    \n",
    "    for annex_num, annex_content in annex_matches:\n",
    "        annexes.append({\n",
    "            \"id\": f\"anx_{annex_num}\",\n",
    "            \"type\": \"annex\",\n",
    "            \"title\": f\"ANNEX {annex_num}\",\n",
    "            \"text\": f\"ANNEX {annex_num}\\n\\n{annex_content.strip()}\"\n",
    "        })\n",
    "    \n",
    "    # Build the document structure\n",
    "    document_data = {\n",
    "        \"title\": regulation_title or title,\n",
    "        \"articles\": articles if articles else None,\n",
    "        \"annexes\": annexes if annexes else None\n",
    "    }\n",
    "    \n",
    "    return document_data\n",
    "\n",
    "def extract_eu_law_text_json(html_content):\n",
    "    \"\"\"\n",
    "    Extract clean, structured data from EU legal document HTML.\n",
    "    Handles both modern and legacy HTML formats.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Remove unwanted elements\n",
    "    for element in soup(['script', 'style', 'link', 'meta', 'hr']):\n",
    "        element.decompose()\n",
    "    \n",
    "    # Check if this is the modern format or legacy format\n",
    "    modern_articles = soup.find_all('div', id=re.compile(r'art_\\d+'))\n",
    "    texte_only_div = soup.find('div', id='TexteOnly')\n",
    "    \n",
    "    if modern_articles or not texte_only_div:\n",
    "        # Use modern parsing\n",
    "        document_data = {\n",
    "            \"header\": extract_header_info(soup),\n",
    "            \"title\": extract_main_title(soup),\n",
    "            \"preamble\": extract_preamble(soup),\n",
    "            \"articles\": extract_articles_json(soup),\n",
    "            \"annexes\": extract_annexes_json(soup),\n",
    "            \"appendices\": extract_appendices_json(soup)\n",
    "        }\n",
    "    else:\n",
    "        # Use legacy parsing for older HTML format\n",
    "        document_data = parse_legacy_format(soup, texte_only_div)\n",
    "    \n",
    "    # Remove None values\n",
    "    return {k: v for k, v in document_data.items() if v is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load to a dataframe the lawsToBeConsidered.csv\n",
    "df = pd.read_csv('lawsToBeConsidered.csv', encoding='utf-8')\n",
    "\n",
    "df_cache_1 = pd.read_csv('cachedLaws_1.csv', encoding='utf-8')\n",
    "df_cache_2 = pd.read_csv('cachedLaws_2.csv', encoding='utf-8')\n",
    "df_cache_3 = pd.read_csv('cachedLaws_3.csv', encoding='utf-8')\n",
    "df_cache_4 = pd.read_csv('cachedLaws_4.csv', encoding='utf-8')\n",
    "df_cache = pd.concat([df_cache_1, df_cache_2, df_cache_3, df_cache_4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this column to store JSON data\n",
    "df['structured_json'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    #Get the CELEX ID\n",
    "    celex_id = row['celex_id']\n",
    "\n",
    "    # If the CELEX ID is not in the cache\n",
    "    if df_cache[df_cache['celex_id'] == celex_id].empty or \\\n",
    "       df_cache[df_cache['celex_id'] == celex_id]['structured_json'].isnull().all() or \\\n",
    "       df_cache[df_cache['celex_id'] == celex_id]['structured_json'].isna().all() or \\\n",
    "       df_cache[df_cache['celex_id'] == celex_id]['structured_json'].apply(lambda x: x == {}).all():\n",
    "                    \n",
    "        encoded_celex_id = url_encode_celex_id(celex_id)\n",
    "        \n",
    "        # Get the HTML content\n",
    "        html = get_html_by_celex_id(encoded_celex_id)\n",
    "        \n",
    "        # Extract structured JSON\n",
    "        structured_json = extract_eu_law_text_json(html)\n",
    "        \n",
    "        # Store JSON\n",
    "        df.at[i, 'structured_json'] = structured_json\n",
    "    else:\n",
    "        try:\n",
    "            cacheJson = df_cache[df_cache['celex_id'] == celex_id]['structured_json'].values[0]\n",
    "\n",
    "            #Convert it to a JSON object\n",
    "            if isinstance(cacheJson, str):\n",
    "                json_data = ast.literal_eval(cacheJson)\n",
    "                df.at[i, 'structured_json'] = json_data\n",
    "\n",
    "            else:\n",
    "                # If it's already a JSON object, just assign it\n",
    "                df.at[i, 'structured_json'] = cacheJson\n",
    "\n",
    "        except:\n",
    "            print(\"Exception parsing JSON for CELEX ID:\", celex_id, \" in row:\", i)\n",
    "            df.at[i, 'structured_json'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the dataframe to a CSV file\n",
    "df.to_csv('lawsWithText.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache generation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset eur-lex-sum (/Users/EduardoGR/.cache/huggingface/datasets/dennlinger___eur-lex-sum/english/1.0.0/616c519b354a68f3396694d1a8dedec40b0fe7b06369a7760999ceaba05b0ab6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20a8e9bbfec4d04a85161f26314a370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Import the dennlinger/eur-lex-sum dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"dennlinger/eur-lex-sum\", \"english\")\n",
    "\n",
    "df_train = dataset['train'].to_pandas()\n",
    "df_test = dataset['test'].to_pandas()\n",
    "df_validation = dataset['validation'].to_pandas()\n",
    "\n",
    "df = pd.concat([df_train, df_test, df_validation], ignore_index=True)\n",
    "\n",
    "df['structured_json'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cache = df.copy()\n",
    "df_cache['structured_json'] = None\n",
    "#Remove all rows from df_cache\n",
    "df_cache = df_cache[0:0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the df to cachedLaws1.csv, cachedLaws2.csv, cachedLaws3.csv, cachedLaws4.csv\n",
    "df_cache_1 = df[:400]\n",
    "df_cache_2 = df[400:800]\n",
    "df_cache_3 = df[800:1200]\n",
    "df_cache_4 = df[1200:]\n",
    "\n",
    "df_cache_1.to_csv('cachedLaws_1.csv', index=False, encoding='utf-8')\n",
    "df_cache_2.to_csv('cachedLaws_2.csv', index=False, encoding='utf-8')\n",
    "df_cache_3.to_csv('cachedLaws_3.csv', index=False, encoding='utf-8')\n",
    "df_cache_4.to_csv('cachedLaws_4.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached laws 1: 0 400\n",
      "Cached laws 2: 400 800\n",
      "Cached laws 3: 800 1200\n",
      "Cached laws 4: 1200 1504\n"
     ]
    }
   ],
   "source": [
    "#Get the lengths of the structured_json\n",
    "i = 0\n",
    "\n",
    "print(\"Cached laws 1:\", 0, len(df_cache_1))\n",
    "i += len(df_cache_1)\n",
    "print(\"Cached laws 2:\", i, len(df_cache_2) + i)\n",
    "i += len(df_cache_2)\n",
    "print(\"Cached laws 3:\", i, len(df_cache_3) + i)\n",
    "i += len(df_cache_3)\n",
    "print(\"Cached laws 4:\", i, len(df_cache_4) + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing laws: 100%|██████████| 1504/1504 [15:05<00:00,  1.66it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing laws\"):\n",
    "    celex_id = row['celex_id']\n",
    "    \n",
    "    if df_cache[df_cache['celex_id'] == celex_id].empty or \\\n",
    "       df_cache[df_cache['celex_id'] == celex_id]['structured_json'].isnull().all() or \\\n",
    "       df_cache[df_cache['celex_id'] == celex_id]['structured_json'].isna().all() or \\\n",
    "       df_cache[df_cache['celex_id'] == celex_id]['structured_json'].apply(lambda x: x == {}).all():\n",
    "        # If the CELEX ID is not in the cache\n",
    "                \n",
    "        encoded_celex_id = url_encode_celex_id(celex_id)\n",
    "        \n",
    "        # Get the HTML content\n",
    "        html = get_html_by_celex_id(encoded_celex_id)\n",
    "        \n",
    "        # Extract structured JSON\n",
    "        structured_json = extract_eu_law_text_json(html)\n",
    "        \n",
    "        # Store JSON\n",
    "        df.at[i, 'structured_json'] = structured_json\n",
    "    else:\n",
    "        try:\n",
    "            cacheJson = df_cache[df_cache['celex_id'] == celex_id]['structured_json'].values[0]\n",
    "\n",
    "            #Convert it to a JSON object\n",
    "            if isinstance(cacheJson, str):\n",
    "                df.at[i, 'structured_json'] = json.loads(cacheJson)\n",
    "            df.at[i, 'structured_json'] = df_cache[df_cache['celex_id'] == celex_id]['structured_json'].values[0]\n",
    "        except:\n",
    "            print(\"Exception parsing JSON for CELEX ID:\", celex_id, \" in row:\", i)\n",
    "            df.at[i, 'structured_json'] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataframe to a csv file\n",
    "df.to_csv('fullLaws.json', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
